<!-- Meta tags -->
<meta name="description" content="Structure-Aware Correspondence Learning for Robust Relative Pose Estimation.">
<meta property="og:title" content="Structure-Aware Correspondence Learning for Relative Pose Estimation"/>
<meta property="og:description" content="A novel method for relative pose estimation using structure-aware keypoints and correspondence estimation modules."/>
<meta property="og:url" content="URL OF YOUR PAPER WEBSITE"/>
<meta property="og:image" content="static/image/your_banner_image.png" />

<meta name="twitter:title" content="Structure-Aware Correspondence Learning for Relative Pose Estimation">
<meta name="twitter:description" content="Robust relative pose estimation with structure-aware keypoints and correspondence estimation.">
<meta name="keywords" content="Pose Estimation, Structure-Aware Learning, Computer Vision, CVPR">

<title>Structure-Aware Correspondence Learning for Relative Pose Estimation</title>

<!-- Paper Title and Authors -->
<h1 class="title is-1 publication-title">Structure-Aware Correspondence Learning for Relative Pose Estimation</h1>
<div class="is-size-5 publication-authors">
  <span class="author-block">
    <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Yihan Chen</a><sup>*</sup>,</span>
  <span class="author-block">
    <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Wenfei Yang</a><sup>*</sup>,</span>
  <span class="author-block">
    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Huan Ren</a>,</span>
  <span class="author-block">
    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Shifeng Zhang</a>,</span>
  <span class="author-block">
    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Tianzhu Zhang</a>,</span>
  <span class="author-block">
    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Feng Wu</a>
  </span>
</div>
<div class="is-size-5 publication-authors">
  <span class="author-block">University of Science and Technology of China, Sangfor Technologies<br>CVPR 2025</span>
  <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
</div>

<!-- Abstract -->
<h2 class="title is-3">Abstract</h2>
<div class="content has-text-justified">
  <p>
    Relative pose estimation offers a powerful approach for generalizing object pose estimation across unseen categories. However, existing methods relying on explicit feature matching often struggle with limited overlapping regions and unreliable feature predictions in unseen or partially visible areas. Inspired by human capability to mentally assemble objects by leveraging structural details, we introduce a Structure-Aware Correspondence Learning framework. Our method comprises two key modules: a structure-aware keypoint extraction module that adaptively selects keypoints reflecting the object structure, guided by an image reconstruction loss; and a correspondence estimation module that leverages self-attention and cross-attention mechanisms to extract robust structure-aware features for establishing accurate 3D correspondences. Extensive evaluations on CO3D, Objaverse, and LineMOD datasets show significant improvements, outperforming state-of-the-art methods with remarkable accuracy.
  </p>
</div>

<!-- Video Teaser Description -->
<h2 class="subtitle has-text-centered">
  Visualization of our robust pose estimation using structure-aware correspondences, highlighting improved accuracy and generalization.
</h2>

<!-- Image Carousel Descriptions -->
<div class="item">
  <img src="static/images/carousel1.jpg" alt="Illustration of structure-aware keypoint extraction"/>
  <h2 class="subtitle has-text-centered">
    Structure-aware keypoint extraction effectively captures object structure.
  </h2>
</div>
<div class="item">
  <img src="static/images/carousel2.jpg" alt="Visualization of correspondence estimation"/>
  <h2 class="subtitle has-text-centered">
    Robust correspondence estimation through intra-image and inter-image attention.
  </h2>
</div>
<div class="item">
  <img src="static/images/carousel3.jpg" alt="Comparative results with state-of-the-art methods"/>
  <h2 class="subtitle has-text-centered">
    Significant accuracy improvement compared to existing methods.
  </h2>
</div>
<div class="item">
  <img src="static/images/carousel4.jpg" alt="Qualitative results on challenging datasets"/>
  <h2 class="subtitle has-text-centered">
    Qualitative performance on diverse and challenging scenarios.
  </h2>
</div>

<!-- Video Presentation Title -->
<h2 class="title is-3">Presentation Video</h2>

<!-- Additional Carousel Title -->
<h2 class="title is-3">Method Demonstrations</h2>

<!-- Poster Section Title -->
<h2 class="title">Conference Poster</h2>

<!-- BibTeX Citation Section -->
<h2 class="title">BibTeX</h2>
<pre><code>
@inproceedings{chen2025structure,
  title={Structure-Aware Correspondence Learning for Relative Pose Estimation},
  author={Chen, Yihan and Yang, Wenfei and Ren, Huan and Zhang, Shifeng and Zhang, Tianzhu and Wu, Feng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025}
}
</code></pre>

<!-- Footer Acknowledgment -->
<div class="content">
  <p>
    This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>, adapted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
    This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
  </p>
</div>
