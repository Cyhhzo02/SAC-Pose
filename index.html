<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SAC-Pose</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Structure-Aware Correspondence Learning for Relative Pose Estimation</h1>

          <div class="is-size-4 publication-authors">
            <span class="author-block">CVPR 2025</span>
          </div>
          <hr>

          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a target="_blank">Yihan Chen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank">Wenfei Yang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank">Huan Ren</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank">Shifeng Zhang</a><sup>3</sup>,</span>
            <span class="author-block">
              <a target="_blank">Tianzhu Zhang</a><sup>1,2</sup></span>
            <span class="author-block">
              <a target="_blank">Feng Wu</a><sup>1</sup></span>
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup><a href="http://ustc.edu.cn/">University of Science and Technology of China</a>,</span>
            <span class="author-block">
              <sup>2</sup><a href="http://www.dsel.cc/">National Key Laboratoray of Deep Space Exploration, Deep Space Exploration Laboratory</a>,</span><br>
            <span class="author-block">
              <sup>3</sup><a href="https://www.sangfor.com.cn/">Sangfor Technologies</a>
          </div> -->

          <div class="is-size-5 publication-authors">
            <!-- Institution Names -->
            <span class="author-block">
              <sup>1</sup>University of Science and Technology of China,</span>
            <span class="author-block">
              <sup>2</sup>National Key Laboratoray of Deep Space Exploration, Deep Space Exploration Laboratory,</span>
            <span class="author-block">
              <sup>3</sup>Sangfor Technologies</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- Arxiv PDF link -->
<!--               <span class="link-block">
                <a href="static/pdfs/SpotPose.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
                </a>
              </span> -->

              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.18671.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
                </a>
              </span>

            <span class="link-block">
              <a href="https://arxiv.org/abs/2503.18671"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>


              <!-- Github link -->
              <span class="link-block">
                <a class="external-link button is-normal is-rounded is-dark"
                   style="background-color: #f0f0f0; color: #aaa; pointer-events: none; cursor: not-allowed;">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video> -->
      <img src="static/images/SAC-Pose_motivation.png" alt="motivation" width="1200"/>
      <h2 class="subtitle has-text-centered">
        <p><strong>(a) Task depiction.</strong> Predict the relative pose <strong>ΔR</strong> between the query and reference images.</p>
        <p><strong>(b) 2D correspondence-based methods</strong> extract keypoints to conduct 2D-2D matching for pose estimation.</p>
        <p><strong>(c) 3D correspondence-based methods</strong> lift 2D features into 3D voxel features and conduct 3D-3D matching for pose estimation.</p>
        <p><strong>(d) Our method</strong> bypasses the feature matching and directly regresses 3D correspondences for pose estimation.</p>
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Relative pose estimation provides a promising way for achieving object-agnostic pose estimation. 
            Despite the success of existing 3D correspondence-based methods, the reliance on explicit feature matching suffers from small overlaps in visible regions and unreliable feature estimation for invisible regions. 
            Inspired by humans' ability to assemble two object parts that have small or no overlapping regions by considering object structure, we propose a novel Structure-Aware Correspondence Learning method for Relative Pose Estimation, which consists of two key modules. 
            First, a structure-aware keypoint extraction module is designed to locate a set of kepoints that can represent the structure of objects with different shapes and appearance, under the guidance of a keypoint based image reconstruction loss. 
            Second, a structure-aware correspondence estimation module is designed to model the intra-image and inter-image relationships between keypoints to extract structure-aware features for correspondence estimation. 
            By jointly leveraging these two modules, the proposed method can naturally estimate 3D-3D correspondences for unseen objects without explicit feature matching for precise relative pose estimation. 
            Experimental results on the CO3D, Objaverse and LineMOD datasets demonstrate that the proposed method significantly outperforms prior methods, i.e., with 5.7°reduction in mean angular error on the CO3D dataset.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper method -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="container">
        <h2 class="title is-3">Method</h2>
        <img src="static/images/SAC-Pose_pipeline.png" alt="pipeline" width="1000"/>
      </div>
    </div>
  </div>
</section>
<!--End paper method -->

<!-- Paper experiment -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="container">
        <h2 class="title is-3">Experiment</h2>
        <img src="static/images/SAC-Pose_experiments.png" alt="exp" width="1000"/>
        <img src="static/images/SAC-Pose_visualization.png" alt="visual" width="700"/>
      </div>
    </div>
  </div>
</section>
<!--End paper experiment -->

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@inproceedings{chen2025structure,
  title={Structure-Aware Correspondence Learning for Relative Pose Estimation},
  author={Chen, Yihan and Yang, Wenfei and Ren, Huan and Zhang, Shifeng and Zhang, Tianzhu and Wu, Feng},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={11611--11621},
  year={2025}
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>
</html>
