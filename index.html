<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Structure-Aware Correspondence Learning</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>Structure-Aware Correspondence Learning for Relative Pose Estimation</h1>
    <p><strong>Authors:</strong> Yihan Chen, Wenfei Yang, Huan Ren, Shifeng Zhang, Tianzhu Zhang, Feng Wu</p>
    <p><strong>Affiliations:</strong> University of Science and Technology of China, National Key Laboratory of Deep Space Exploration, Sangfor Technologies</p>
  </header>

  <section id="abstract">
    <h2>Abstract</h2>
    <p>We propose a <strong>Structure-Aware Correspondence Learning</strong> framework for relative pose estimation of unseen objects, which avoids explicit feature matching. Our method consists of a <em>structure-aware keypoint extraction module</em> and a <em>structure-aware correspondence estimation module</em>. By directly regressing 3D-3D correspondences using structural information, our approach significantly outperforms prior methods on CO3D, Objaverse, and LineMOD datasets, achieving up to <strong>5.7&deg; reduction in mean angular error</strong>. This design offers an effective and efficient solution for object-agnostic pose estimation in robotic and AR/VR applications.</p>
  </section>

  <section id="video">
    <h2>Video</h2>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/YOUR_VIDEO_ID" frameborder="0" allowfullscreen></iframe>
  </section>

  <section id="paper">
    <h2>Paper</h2>
    <ul>
      <li><a href="https://arxiv.org/abs/2503.18671" target="_blank">[arXiv] Structure-Aware Correspondence Learning for Relative Pose Estimation</a></li>
      <li>Accepted to <strong>CVPR 2025 (under review)</strong></li>
    </ul>
  </section>

  <section id="code">
    <h2>Code</h2>
    <p>Code will be available soon at: <a href="https://github.com/YOUR_GITHUB_REPO" target="_blank">GitHub Repository</a></p>
  </section>

  <section id="results">
    <h2>Results</h2>
    <ul>
      <li><strong>CO3D:</strong> mAE &darr; <span style="color:green;">14.2&deg;</span>, Acc@30&deg; &uarr; 93.6%, Acc@15&deg; &uarr; 80.2%</li>
      <li><strong>Objaverse:</strong> mAE &darr; 15.3&deg;, Acc@30&deg; &uarr; 90.3%</li>
      <li><strong>LineMOD:</strong> mAE &darr; 27.2&deg;, Acc@30&deg; &uarr; 76.2%</li>
    </ul>
    <p>Compared to 2D and 3D correspondence-based baselines (e.g., LoFTR, DVMNet), our method is more robust under low-overlap and unseen-category scenarios.</p>
  </section>

  <section id="highlights">
    <h2>Highlights</h2>
    <ul>
      <li>üìå Structure-aware keypoint extraction using learnable queries and image reconstruction loss.</li>
      <li>ü§ù Direct regression of 3D-3D correspondences without feature matching.</li>
      <li>‚ö° Efficient and generalizable: outperforms prior methods with fewer FLOPs and better accuracy.</li>
    </ul>
  </section>

  <section id="figures">
    <h2>Figures</h2>
    <p><img src="assets/overview.png" alt="Overview Diagram" width="90%"></p>
  </section>

  <section id="contact">
    <h2>Contact</h2>
    <p>If you have any questions, feel free to contact <a href="mailto:yangwf@ustc.edu.cn">yangwf@ustc.edu.cn</a> or <a href="mailto:tzzhang@ustc.edu.cn">tzzhang@ustc.edu.cn</a></p>
  </section>

  <footer>
    <p>&copy; 2025 Structure-Aware Correspondence Learning Project</p>
  </footer>
</body>
</html>
